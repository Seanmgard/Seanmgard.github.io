[
  {
    "objectID": "Projects/2023-01-13 GTS (Big Data Bowl 2024)/index.html",
    "href": "Projects/2023-01-13 GTS (Big Data Bowl 2024)/index.html",
    "title": "NFL Big Data Bowl 2024: Gridiron Tackle Superiority (GTS)",
    "section": "",
    "text": "Introduction\nThis project presents Gridiron Tackle Superiority (GTS), a metric for calculating the expected likelihood of making a tackle. The model employs a binomial logistic regression that considers factors like proximity to the ball, speed, and angle of pursuit to identify an individual player’s likelihood of making a tackle at every moment of the play. The implications of this metric are two-fold: First, GTS shows how tackle probabilities change in real time. The model demonstrates how split-second decisions by both the offense and defense can change those probabilities, and how players may adjust to those decisions. Furthermore, the analysis can be mapped to actual tackle statistics, offering a method of measuring pure tackling ability. In theory, a player that records more tackles than expected could be judged to have stronger pure tackling ability after accounting for circumstances out of their control like offensive play calls, defensive formations, and other factors.\n\n\nMethodology\nThe model utilizes a logistic regression that seeks to measure tackling probability as a function of five independent variables – Proximity to Ball, Relative Angle to Ball, Speed, Acceleration, and the Number of Players Near the Ball. Tackle probabilities are calculated across every frame of play for every defensive player on the field. The equation is defined as:\n\n\n\n\n\nProximity to Ball\nProximity to Ball is measured as the distance between the player and the ball in each frame of the play. The coordinates for both the ball and each defensive player are plotted on the field and the distance between both is measured with the formula for Euclidean distance, as shown below.\n\n\n\n\n\nWhere:\n- (x1, y1) represent the coordinates for each player\n- (x2, y2) represent the coordinates for the ball.   \nAs each player moves closer to (or further from) the ball in each frame, those coordinates change, as does the ‘Proximity to Ball’ metric from the logistic regression.\nAngle to Ball\nA player’s relative angle to the ball is measured in a two-step process. First, we simply observe a player’s orientation on the field at any given moment. From there, we measure the angle between their orientation and the position of the ball. This helps determine if the player is primed to make a play on the ball and the degree to which that changes throughout the play. The chart below outlines this process in more detail. We define a 2-D cartesian coordinate system (in this case, represented by the football field), and calculate the angle of the player relative to the ball with a two-argument arctangent function.\n\n\n\n\n\n\n\n\n\n\nSpeed and Acceleration\nSpeed is measured in terms of distance traveled in yards/second, while acceleration is measured in yards/second^2. This is measured at each frame of the play and impacts both tackle probability in the moment, as well as how that probability changes as the play goes on.\nPlayers Near Ball\nThis variable is defined as the number of players within 3 yards of the ball at every point of the play. As this value increases, the expected likelihood of surrounding players contributing to a tackle increases. A value of 3 yards was chosen somewhat arbitrarily, so there could be some refinement to this metric.\n\n\nExample Play\nThe play below is from Week 2 of the 2022 season. This play occurs on first and ten with 1:26 left in the 1st quarter, as Aaron Rodgers completes a pass to Sammy Watkins for a gain of 24 yards. Throughout the play, the expected likelihood of completing a tackle changes as each player’s proximity to the ball, angle to the ball, speed, acceleration, and number of nearby teammates changes. In the end, Eddie Jackson makes the tackle, though both Jaquan Brisker and Kyler Gordon demonstrate a high probability of making a tackle moments before as well.\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating Gridiron Tackle Superiority\nAfter defining these variables, the model is run and determines the individual tackle probabilities for all players in every frame of data for the first 9 weeks of the 2022 season. While the individual tackle probabilities are informative within a given play, we can also calculate the average likelihood of making a tackle throughout the entire season. From there, we’re able to determine the expected number of tackles for each defensive player and compare that to the actual tackles observed in the season. We define those metrics with the following equations.\nSeason average tackle probability is calculated as the sum of tackle probabilities for each frame a player was present on the field during the season divided by the sum of the total number of frames played. Frames played is found by taking the number of plays present for each player multiplied by the number of frames associated with those plays.\n\n\n\n\n\nExpected tackles is a simple metric that can be found by taking the total season average tackle probability calculated in Step 1 divided by the total number of snaps played. This value represents the number of tackles anticipated for the season based on the calculated season average tackle probabilities and the number of opportunities the player had to participate in a tackle.\n\n\n\n\n\nLastly, we find Gridiron Tackle Superiority (GTS) by taking the index of the Actual Number of Tackles divided by the Expected Number of Tackles. This represents performance relative to expectations and can be thought of as pure tackling ability.\n\n\n\n\n\n\n\nRanking\nBelow are the GTS Rankings for the top 10 players in each position group. GTS presents a framework that can measure situation-independent tackling ability for each player. Expected tackles are measured as a function of both situational tackle probability, as well as the actual amount of time played on the field. This provides a more unique method of tracking tackling ability between starters and backups. While the model does set a threshold of at least 50 snaps played to prevent outlier performance, the rankings are highly inclusive. The charts outlined below segment GTS across three primary player groups - Defensive Line, Linebacker, and Secondary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThis submission proposes a novel metric to measure both tackle probability throughout the duration of a play, as well as overall tackle skill of individual players. Traditional metrics like solo tackles and assists understate the degree to which external circumstances impact the probability of making a tackle. Other more advanced defensive metrics also fall short of measuring pure tackling ability. Here, we control for some of those factors and add helpful context to observed tackle statistics.\nThis metric does leave room for future modifications. For example, GTS does not consider the individual skill of surrounding teammates, which may impact tackle probabilities. In addition, there may be room to normalize the metric further to reduce some observed discrepancies across position groups.\nThat said, GTS and the inter-play tackle probabilities calculated here make significant progress in evaluating tackle ability.\n\n\nAppendix\nCode for this project is available at https://github.com/Seanmgard/Big-Data-Bowl-2024"
  },
  {
    "objectID": "posts/2024-04-08-Investigating-Beatles/index.html",
    "href": "posts/2024-04-08-Investigating-Beatles/index.html",
    "title": "Investigating The Beatles with Machine Learning",
    "section": "",
    "text": "During my sophomore year in college, I enrolled in a class titled ‘Music of the Beatles’, which was taught by the legendary Glenn Gass. Glenn spent over 40 years teaching at Indiana University and famously developed the first history of rock and roll course at a major university. The course was taught late in the evening on Tuesdays and Thursdays and took place in an old lecture hall at the center of campus, where Glenn would spend almost two hours walking us through the history of the Beatles. We listened to every album from that era, absorbing the stories and inspirations behind every song, and appreciating their unique brilliance.\nIt seems counterintuitive to use something as emotionless as machine learning to rank the emotional range of a piece of music. And it’s not necessarily wrong to have a negative reaction to the idea — Some people on Reddit certainly did. I posted one of the charts from this analysis to subreddits like r/Beatles and it generated some mixed reactions. ‘Anxious-Raspberry-54’ said, “I could care less about a computer analyzing songs. It’s art”, while ’Clutch-Cargo52 said, “Energy comes from the heart and the core of a person, not fingers and throats…and especially not from a computer program.” And you know what? They’re right! But I think this analysis is interesting, in part, because it does actually account for many of that things that tend to correlate with our feelings for a song. In other words, this is an attempt to measure song characteristics (yes, with the help of computers) as a human might perceive them. This obviously requires a robust and validated model with multiple different data points to successfully execute. Thankfully, that’s precisely what Spotify’s developer team has created (with some help). I’ll start with an example that shows that in action.\n\nOverview of The Beatles Discography\nOne of the audio attributes measured by Spotify is “Energy”, which is meant to represent the overall intensity and activity of a song. Their Developer Documentation says, “Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset range, and general entropy”. Of course, this does not consider the song’s origin, specific lyrics, or other attributes that could contribute to the energy one feels while listening to a song. That said, it does contain some key attributes that highly correlate to perceive energy from listeners. The chart below breaks down the energy characteristics of every album in their discography (minus Magical Mystery Tour and Yellow Submarine). It contains some interesting findings.\n\n\n\n\n\nOn average, the Beatles displayed far more energy earlier in their career. In fact, their first five studio releases represent the five most energetic albums of their entire discography – Though their second album, With The Beatles, is meaningfully more energetic than numbers two through five. This is likely not surprising when you listen to tracks like I Wanna Be Your Man, Little Child, and Money (That’s What I Want), all of which fall in the top 10 of the entire Beatles catalog of 200+ songs.\nThough later albums display less energy on average, that doesn’t necessarily mean the band could no longer bring it. The two most energetic tracks ever released by the Beatles were ‘Back In The U.S.S.R’ and ‘Polythene Pam’, which were on the White Album and Abbey Road respectively. This more reflects the range present as the band became more mature, a dynamic that becomes clear from the chart above. The points are far more sporadic on something like the White Album, which included low-energy tracks like Julia and Blackbird alongside high-energy tracks like Helter Skelter.\n\n\nBreaking Down the Songs\nThe range of the Beatles is evident from the first chart. The plot below highlights that range in more detail. The first table ranks the albums by overall energy, while the second and third tables show the highest and lowest ranking songs by energy on each album.\n\n\n\n\n\nThese dynamics also break down in interesting ways across each member of the band. Lennon and McCarthy composed over 80% of the Beatles catalog, either in an individual capacity or through collaboration. Paul arguably displays more energetic range throughout his contributions, but overall, ranked the lowest out of all four Beatles. Combined Lennon/McCarthy tracks displayed the highest energy by a meaningful margin, led by songs like ‘Little Child’ and ‘I Wanna Be Your Man’. George’s contributions tended to congregate more towards the middle in terms of ‘energy’, as did Ringo, though his his role as a ‘composer’ was minimal – But I did love ‘Octopus’s Garden’.\n\n\n\n\n\n\n\nAre Any of These Measures Correlated?\nSpotify doesn’t just track Energy – Their team calculates various audio features for tens of millions of different songs. These audio features include a wide range of metrics that can be used to measure a song’s structure and musical content.\n\nEnergy\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\nDanceability\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\nInstrumentalness\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\nLoudness\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\nSpeechiness\nSpeechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\nTempo\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\nValence\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\nThese features contain unique information about songs, but also share common characteristics in certain cases. The chart below shows how some of these measures correlate with energy for songs in the Beatles discography.\n\n\n\n\n\nIn general, most audio features show little correlation with energy for Beatles songs. As the Spotify description notes, energetic tracks tend to be associated with things like perceived loudness or intensity, which perhaps explains why ‘loudness’ and ‘valence’ were the two metrics most correlated with energy. With valence, we observe an R-Squared of 0.25, while loudness is 0.50. That said, the observations across all measures are clearly sporadic.\n\n\nWhat Goes Into a Rating?\nIn 2014, Spotify acquired ‘The Echo Nest’ in a $100M deal that provided it with new audio tracking capabilities. The core methodology used to calculate Spotify’s ‘Audio Features’ originated with this acquisition. Interestingly, the concepts leveraged by Echo Nest originated even earlier with a dissertation from Tristan Jehan titled, ‘Creating Music by Listening’, which was published in 2005. The entire thesis can be found here, but a brief overview of the Abstract can be found below\n\nMachines have the power and potential to make expressive music on their own. This thesis aims to computationally model the process of creating music using experience from listening tot examples. Our unbiased signal-based solution models the life cycle of listening, composing, and performing, turning the machine into an active musician, instead of simply an instrument. We accomplish this through an analysis-synthesis technique by combined perceptual and structural modeling of the musical surface, which leads to a minimal data representation.\nWe introduce a music cognition framework that results from the interaction of psychoacoustically grounded casual listening, a time-lag embedded feature representation, and perceptual similarity clustering. Out bottom-up analysis intends to be generic and uniform by recursively revealing metrical hierarchies and structures of pitch, rhythm, and timbre. Training is suggested for top-down unbiased supervision, and is demonstrated with the prediction of downbeat. This musical intelligence enables a range of original manipulations including song alignment, music restoration, cross-synthesis or song morphing, and ultimately the synthesis of original pieces.\n\nWhile the algorithms used have become more complex over time, the core principles outlined by Jehan are still a core part of those calculations. Spotify’s documentation unfortunately does not break down how each of the current metrics are precisely calculated. Your best bet for understanding those calculations would be Jehan’s original paper. Fortunately, there are still some sporadic examples across the web that dive into these calculations a bit deeper than the primary documentation.\nFor example, we have some information about the probability distributions of these metrics. The features are not normally distributed and some like instrumentalness, acousticness, and speechiness display a significant right skew. Mark Koh, an Engineer at Spotify, outlined some of this in a presentation given at the Monthly Music Hackathon NYC in 2018. The distribution breakdowns can be found in the chart below.\n\nMark’s presentation goes into a lot more detail on some of these metrics (which, as a non-musician, is a bit beyond my level of understanding). You can view more from that presentation here.\n\n\nSummary\nThis is not meant to be an objective ranking of the quality of the Beatles catalog. Their discography inspires all sorts of emotions in listeners, and the relative quality (or any metric for that matter) is in the eye of the beholder. Nonetheless, it’s interesting to see the ways researchers are able to quantify and measure certain attributes of songs, even from bands as iconic as the Beatles. This approach goes beyond traditional subjective interpretation and offers new insights into the construction and impact of different songs. While it doesn’t diminish the personal and emotional connections that fans have with the music, it adds a layer of appreciation for the craftsmanship and innovation that goes into creating these tracks. More than anything, I think it helps quantify many of the lessons Glenn taught me in his class years ago."
  },
  {
    "objectID": "posts/2023-09-22-High-School-Recruit-Analysis/index.html",
    "href": "posts/2023-09-22-High-School-Recruit-Analysis/index.html",
    "title": "Analyzing the High School Recruit to NFL Draft Pick Pipeline",
    "section": "",
    "text": "Each year, ESPN ranks the top 300 high school football recruits in the country. Players are evaluated on a variety of metrics to determine how they compare vs. their peers by state, position, and across the nation. These lists are usually a great indicator of how impactful players are for their varsity team. For example, Malachi Nelson, the #1 ranked player in the class of 2023, passed for 2,898 yards and 35 touchdowns this past season. Next year he’ll be attending the University of Southern California (USC), where fans and analysts will be evaluating whether he’s the next future NFL star. We’ve seen plenty of examples of that in recent history. Just five years ago, Justin Fields and Trevor Lawrence, now rising QBs in the NFL, ranked as the number 1 and number 2 players in the class of 2018.\nAs I mentioned, the top 300 list is meant to reflect how talented players are at a given point in time. But how well does it predict future success? One would think recruiting grades correlate well with eventual draft likelihood, but is that actually the case? Furthermore, do those trends hold across all positions? In this analysis, I set out to start answering those questions.\nFirst, we need to gather all the necessary data. I initially built a web scraper in Python to pull data for the top high school football recruits between 2006 and 2023 from ESPN.com. I used the pandas library to read the HTML tables on each webpage, clean the data, and extract it to a separate excel file. I then ran some simple excel functions to combine that dataset with the nflverse-data repository on Github. From that, we were left with a list that contains 4,350 players. I then used ggplot2 in R to run the analysis below.\n\nRanking Successful Players\nLet’s start with a general view of all draftees, and how they ranked in high school. This table outlines the high school ranking of each player that was drafted between 2009 and 2023. This list only includes draftees that were ranked in high school, and you may be surprised to learn that most draft picks in any given year weren’t ranked among the top 150 players of their recruiting class. With the subset of players that were, in fact, ranked that high, we see that those ranked in the top 50 did indeed have a higher likelihood of being drafted than those ranked lower. That said, beyond the top 50, those differences become far less pronounced.\n\nThe first takeaway here may be that truly top talents are easy to evaluate from an early age, but measuring performance beyond that point becomes more difficult. The dynamic is even more interesting if we break things down by position.\n\nA few interesting themes emerge here. First, skill positions like WR tend to comprise a meaningful selection of the top ESPN recruits each year. This makes sense if we think about the depth chart of any given football team. Unlike many other positions (ex: FB), there will often be 2-4 receivers on the field at a time, which makes the position itself that much more valuable.\nThat said, we still see some interesting dynamics within positions. Prior to this analysis, my hypothesis was that certain positions may have a higher likelihood of being drafted after ranking highly in high school than others. Instead, the themes across each of these are relatively consistent. I suppose this makes sense too. Think for a moment about five of the top QBs in the league at this moment (Patrick Mahomes, Jalen Hurts, Joe Burrow, Josh Allen, and Justin Herbert). Of those players, only Joe Burrow was ranked among the Top 300 high school recruits coming into college, and he ranked 298th. It’s hard to predict these things.\n\n\nTalent Evaluation by School\nI was also curious whether the colleges these recruits were choosing were any better at picking future NFL draft picks than others. The chart below outlines the number of high school recruits that committed to each school over the same time period mentioned above and excludes any schools that did not recruit at least 25 top 300 players. In general, powerhouse schools like Alabama, Georgia, and LSU recruited significantly more top-rated players than other schools.\n\nBefore making any conclusions from this data set, I think it’s important to note a few important factors here. 1.) Some schools may have better systems and development programs than others, which can significantly impact a recruit’s ability to reach the NFL. 2.) Some schools may pass on clear-cut future NFL stars due to their existing depth chart. An obvious example can be found in 2018. It was not in the interest of the teams or players for one school to receive commits from both Justin Fields and Trevor Lawrence. Various situational dynamics have existed over the past several years like this. Beyond that, it’s also often in the interest of an incoming recruit to choose a school where he’s more likely to make an immediate impact, rather than waiting behind a stacked Alabama roster. 3.) This list is based on the schools that players committed to, not the schools they attended when they were drafted. For example, Cam Newton is technically listed as a recruit of ‘Florida’ in this dataset, rather than Auburn, where he won a Heisman and a national championship. While some interesting exploration can be done on that data, I wanted to isolate any transfer-portal related dynamics, so that we can evaluate a school’s ability to choose recruits.\nLastly, it’s also just worth mentioning that schools don’t definitively choose their recruits. They extend offers to athletes, who then have the choice of choosing from a selection of different schools. Perhaps in a future analysis, we can investigate a more detailed dataset that includes “offers extended” from each school, rather than just “offers accepted.”\nFrom the table above, we can see that the data doesn’t necessarily show any clear trends. While schools like Alabama and Georgia generated more future NFL draft picks than other schools, they also tended to recruit more Top 150 high school players. In other words, their ability to choose and develop future NFL talent isn’t necessarily higher than their peers on a relative basis. A notable exception seems to be Texas, who generates a significantly lower % of drafted players than its peers.\n\n\nConclusion\nThis dataset may confirm some priors about high school recruiting and perhaps dispel some notions about talent evaluation in football. Here are some takeaways:\n1.) There is a correlation between a player’s high school ranking and their likelihood of being drafted. That said, the correlation decreases for players ranked outside the top 50.\n2.) There isn’t a clear theme between a player’s position and their likelihood of being drafted. There are certainly more WRs drafted than say, ILBs, but there are also more WRs among the top ranked players to begin with.\n3.) There isn’t a clear trend in talent evaluation among the top schools. Powerhouse schools attract more top recruits, and as we know, those players have a higher likelihood of being drafted. But the percentage of players drafted across the top ~20 schools is consistent.\nThis is part 1 of an ongoing series where I analyze the high school recruit to NFL draft pick pipeline. In the future, I hope to expand this dataset and continue investigating trends among players and schools."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sean Gardner",
    "section": "",
    "text": "About Me\nI’m a Quantitative Research Associate on the Portfolio Research team at Northern Trust. Before my time at Northern Trust, I was also on the Forecasting team at NielsenIQ, where I specialized in data analytics and forecasts for new product innovations. I graduated from Indiana University in 2019 with a degree in Economics.\nI have a passion for data and wanted to set up a space where I can post about things I’ve been working on in my free time. I want this to be a place where I can keep track of my progress in data visualization and modeling, and hopefully learn from others as well.\nPlease reach out through email or social for any inquiries!"
  },
  {
    "objectID": "posts/2023-09-23-Employment-Situation/index.html",
    "href": "posts/2023-09-23-Employment-Situation/index.html",
    "title": "A Review of Employment Data (March 2023)",
    "section": "",
    "text": "On March 9th, the Bureau of Labor Statistics (BLS) released its Job Openings and Labor Turnover report (also known as \"JOLTS\") for January 2023. The report noted a slight decline in the number of job openings for the month, largely driven by construction, accommodation and food services, and finance and insurance.\nThe following day, we received the Employment Situation, which showed that non-farm payroll rose by 311,000 for the month of February, with the unemployment rate (3.6%) and unemployed persons (5.9 million) rising as more people entered the labor force. This comes alongside a plethora of other reports, along with commentary from market analysts, who are trying to project where the employment situation might go.\nWe live in an interesting era, where traditional employment tells conflicting stories. That data can even be bolstered, or contradicted, by ongoing data released from companies like LinkedIn and Indeed. While data from those sites may paint a less comprehensive picture, it still provides helpful insight on the overall macroeconomic situation. It also causes confusion when there are gaps in methodologies, timing, and economic metrics that tell varying stories.\n\nWhat does the data show?\nFirst, let’s look at some of the employment metrics. I’ve broken the data based on the various reports and tried to tie the threads together where applicable. Let’s start with the ADP Employment Report.\n\nAt a national level, ADP nonfarm payrolls rose by 242,000, beating estimates of 200,000. That said, as Pantheon Macro noted on Wednesday, ADP developed a new methodology for their employment calculations in August of 2022, and has only released 6 payroll numbers since then. Of those, four have undershot official payrolls. We still don’t know whether that trend will hold, of course, but it’s important to remember these estimates can be inconsistent.\nFurther data from the report shows a more nuanced picture.\n\nThe chart above shows the week-to-week change in private employment by industry since mid-2021. In the month of February, job growth was led, in part, by growth in financial activities (+62,000) and manufacturing (+43,000). Not shown in the chart above is Leisure and Hospitality, which also showed job growth of +83,000 as well. Beyond that, education and health services (+35,000), natural resources and mining (+25,000), and other services (+34,000) showed growth. Declines were observed in professional and business services (-36,000) and construction (-16,000). Net- Overall employment seems to be positive, though there is some underlying weakness below the surface. This becomes clearer if we break the report down by size of business.\nPositive trends were driven entirely by businesses with more than 50+ employees. Small businesses (those with 19 or less employees) saw employment contract by -56,000 in the month of February. On the opposite end of the spectrum, employment by businesses larger than 500 employees grew by +160,000.\n\nThis has been the theme in private employment going back to the start of 2022. Since then, businesses with 50-249 employees have seen the largest growth (~2.3 million jobs added), while those with 1-19 employees have consistently reduced employment over the past 12 periods of data (-577,000 jobs lost).\nNext, let’s look at the JOLTS report from last Wednesday. Here, we saw the number of job openings decrease in January, as reported by the BLS. Though the data here lags the ADP Report by about a month, similar themes emerged as job openings in construction (-240,000) and finance + insurance (-100,000) contributed to the decline in overall openings (-410,000). Some increases were observed in areas like services (+95,000) and trade, transportation, and utilities (+51,000). In general, these declines were observed primarily in the West and Midwest.\n\n\nLooking at total separations, quits declined to 2.5%, down from 2.6% the month prior, while layoffs and discharges were relatively stable at 1.1%. The decline in quits was primarily driven by Professional and business services, which fell from 719,000 in December to 498,000 in January. Similarly, layoffs and discharges increased by +191,000 in January to 1.7 million, up from ~1.5 million in December. This was again largely due to negative dynamics among professional and business services workers. Some have said these trends are indicative of continued labor market cooling, but outside of professional and business services, shifts have been very slow.\nHiring and separations in the JOLTS report seems to differ vs. some of the trends observed in the ADP Employment Report, though again, there is a difference in the time periods measured. Businesses with 1-9 employees have shown the strongest growth over the past few months, up +68,000 since October 2022. While total private employment is up over that same time frame (+251,000), businesses with 50-249 have driven most of that, with an increase of +129,000. What sticks out from this dataset is the change in job openings over the same period. Businesses with 1-9 employees increased openings by +355,000, while businesses with 50-249 employees were relatively flat. Businesses with 250-999 employees reduced hiring by -159,000. This will be a trend worth monitoring.\n\nAs mentioned at the beginning, the release of the Employment Situation for February 2023 showed an increase in the unemployment rate, as more people entered the labor force. Here, the data shows a bit of a mixed story. For those looking for signs of labor market cooling, we see some examples of softening in areas like average hourly wages, hours worked, and the overall unemployment rate. That said, we still see moderate growth, with little sign of an increase in the rate of cooling on the horizon. Leisure and hospitality led the increase in employment, adding +105,000 jobs in February, along with Food services, which added +70,000 jobs, and retail trade, which rose +50,000.\nWhile it’s difficult to draw strong conclusions about this report, there are still some interesting themes in the data. First, the unemployment rate has differed among different ages and levels of educational attainment. Since February 2022, the unemployment rate for those with less than a high school diploma, a group that admittedly only makes up ~6% of the total labor force, has significantly increased from 4.5% in January to 5.8% in February. Meanwhile, the unemployment rate has stayed relatively stagnant for high school graduates without a degree and those with a bachelor’s degree or higher, though it did increase from 2.9 to 3.2% for those with some college or associate degree.\n Differences emerge if we look specifically at age as well. Since February of 2022, unemployment has remained steady for most age cohorts except for those aged 35 to 44 years old, which has seen a decrease in the unemployment rate from 3.3% to 2.6%, and 55 years and over, which has seen a decline in the unemployment rate from 2.9% to 2.5%. We’ve even seen some interesting movements since just last month.\n\nThe unemployment rate for those aged 16 to 17 years old has declined from 10.9% to 9.0%, while it climbed to 12.1% from 10.0% for those aged 18 to 19 years old. This is likely driven by some movements in the labor force participation rate, which saw a significant decline for the former cohort, and a significant rise for the latter. That said, trends have been steady overall for most of the prime age working population.\n\n\nHow do these results square with other miscellaneous sources?\nIt’s mixed! On March 6th, LinkedIn released its Workforce Report for March 2023, which outlined some interesting trends. In general, the report showed substantial declines in hiring (-6.5%), which was the largest seen since April 2020. This marked the 10th consecutive month of declines, bringing the overall Y-Y hiring decline to -27.9%. Industries like Real Estate (+4%), Education (+2%), and Accommodation (+1%) showed the strongest month-to-month increases, while Manufacturing (-10%), Technology, Information, and Media (-10%), and Construction (-9%) showed the largest declines. The LinkedIn hiring rate looks at the number of hires divided by LinkedIn membership. While it may not necessarily be predictive, it still provides another helpful data point for employment. Most industries have declined since last year, though Tech (-47), Professional Services (-31), and Holding Companies (-31) lead the charge.\n\n\nIndeed also tries to monitor employment opportunities through its ‘Job Postings Index’. The index is set to 100 for February 2020 and uses a 7-day moving average to monitor employment opportunities. This data shows a significant decline over the past few months.\n\n\n\nSummary\nWe have conflicting data from various sources, but overall, the labor market seems resilient with some underlying cracks below the surface. The Fed seems to be chipping away at employment to bring inflation down, but the process so far has been slow. If I had to make a prediction, I think we will continue to see the Fed raise rates to ~5.5-6% by the end of the year. There are whispers of coming layoffs among larger companies, particularly in industries like tech and consulting, so my expectation is an unemployment rate around 4.5-5% by the end of the year, with uneven impacts across different age and educational cohorts."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Sean Gardner",
    "section": "",
    "text": "Investigating The Beatles with Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2024\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing the High School Recruit to NFL Draft Pick Pipeline\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nA Review of Employment Data (March 2023)\n\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Sean Gardner",
    "section": "",
    "text": "NFL Big Data Bowl 2024: Gridiron Tackle Superiority (GTS)\n\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2024\n\n\n\n\n\n\nNo matching items"
  }
]