<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-04-08">

<title>Sean Gardner - Investigating The Beatles with Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-E5T5MLDTMN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-E5T5MLDTMN', { 'anonymize_ip': true});
</script>


<meta name="twitter:title" content="Sean Gardner - Investigating The Beatles with Machine Learning">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="../..\posts/2024-04-08-Investigating-Beatles/images/Beatles_Plot.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Sean Gardner</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Visualizations.html" rel="" target="">
 <span class="menu-text">Visualizations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../Projects.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview-of-the-beatles-discography" id="toc-overview-of-the-beatles-discography" class="nav-link active" data-scroll-target="#overview-of-the-beatles-discography">Overview of The Beatles Discography</a></li>
  <li><a href="#breaking-down-the-songs" id="toc-breaking-down-the-songs" class="nav-link" data-scroll-target="#breaking-down-the-songs">Breaking Down the Songs</a></li>
  <li><a href="#are-any-of-these-measures-correlated" id="toc-are-any-of-these-measures-correlated" class="nav-link" data-scroll-target="#are-any-of-these-measures-correlated">Are Any of These Measures Correlated?</a></li>
  <li><a href="#what-goes-into-a-rating" id="toc-what-goes-into-a-rating" class="nav-link" data-scroll-target="#what-goes-into-a-rating">What Goes Into a Rating?</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Investigating The Beatles with Machine Learning</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 8, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>During my sophomore year in college, I enrolled in a class titled ‘Music of the Beatles’, which was taught by the legendary Glenn Gass. Glenn spent over 40 years teaching at Indiana University and famously developed the first history of rock and roll course at a major university. The course was taught late in the evening on Tuesdays and Thursdays and took place in an old lecture hall at the center of campus, where Glenn would spend almost two hours walking us through the history of the Beatles. We listened to every album from that era, absorbing the stories and inspirations behind every song, and appreciating their unique brilliance.</p>
<p>It seems counterintuitive to use something as emotionless as machine learning to rank the emotional range of a piece of music. And it’s not necessarily wrong to have a negative reaction to the idea — Some people on Reddit certainly did. I posted one of the charts from this analysis to subreddits like r/Beatles and it generated some mixed reactions. ‘Anxious-Raspberry-54’ said, “I could care less about a computer analyzing songs. It’s art”, while ’Clutch-Cargo52 said, “Energy comes from the heart and the core of a person, not fingers and throats…and especially not from a computer program.” And you know what? They’re right! But I think this analysis is interesting, in part, because it <em>does</em> actually account for many of that things that tend to correlate with our feelings for a song. In other words, this is an attempt to measure song characteristics (yes, with the help of computers) <em>as a human might perceive them</em>. This obviously requires a robust and validated model with multiple different data points to successfully execute. Thankfully, that’s precisely what Spotify’s developer team has created (with some help). I’ll start with an example that shows that in action.</p>
<section id="overview-of-the-beatles-discography" class="level3">
<h3 class="anchored" data-anchor-id="overview-of-the-beatles-discography">Overview of The Beatles Discography</h3>
<p>One of the audio attributes measured by Spotify is “Energy”, which is meant to represent the overall intensity and activity of a song. Their Developer Documentation says, “Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset range, and general entropy”. Of course, this does not consider the song’s origin, specific lyrics, or other attributes that could contribute to the energy one feels while listening to a song. That said, it does contain some key attributes that highly correlate to perceive energy from listeners. The chart below breaks down the energy characteristics of every album in their discography (minus Magical Mystery Tour and Yellow Submarine). It contains some interesting findings.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../..\posts/2024-04-08-Investigating-Beatles/images/Beatles_Plot.png" class="img-fluid figure-img" width="677"></p>
</figure>
</div>
<p>On average, the Beatles displayed far more energy earlier in their career. In fact, their first five studio releases represent the five most energetic albums of their entire discography – Though their second album, With The Beatles, is meaningfully more energetic than numbers two through five. This is likely not surprising when you listen to tracks like I Wanna Be Your Man, Little Child, and Money (That’s What I Want), all of which fall in the top 10 of the entire Beatles catalog of 200+ songs.</p>
<p>Though later albums display less energy on average, that doesn’t necessarily mean the band could no longer bring it. The two most energetic tracks ever released by the Beatles were ‘Back In The U.S.S.R’ and ‘Polythene Pam’, which were on the White Album and Abbey Road respectively. This more reflects the range present as the band became more mature, a dynamic that becomes clear from the chart above. The points are far more sporadic on something like the White Album, which included low-energy tracks like Julia and Blackbird alongside high-energy tracks like Helter Skelter.</p>
</section>
<section id="breaking-down-the-songs" class="level3">
<h3 class="anchored" data-anchor-id="breaking-down-the-songs">Breaking Down the Songs</h3>
<p>The range of the Beatles is evident from the first chart. The plot below highlights that range in more detail. The first table ranks the albums by overall energy, while the second and third tables show the highest and lowest ranking songs by energy on each album.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../..\posts/2024-04-08-Investigating-Beatles/images/Beatles_Top_Songs_Chart.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>These dynamics also break down in interesting ways across each member of the band. Lennon and McCarthy composed over 80% of the Beatles catalog, either in an individual capacity or through collaboration. Paul arguably displays more energetic <em>range</em> throughout his contributions, but overall, ranked the lowest out of all four Beatles. Combined Lennon/McCarthy tracks displayed the highest energy by a meaningful margin, led by songs like ‘Little Child’ and ‘I Wanna Be Your Man’. George’s contributions tended to congregate more towards the middle in terms of ‘energy’, as did Ringo, though his his role as a ‘composer’ was minimal – But I did love ‘Octopus’s Garden’.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../..\posts/2024-04-08-Investigating-Beatles/images/Songwriter_Chart_1.png" class="img-fluid figure-img" width="688"></p>
</figure>
</div>
</section>
<section id="are-any-of-these-measures-correlated" class="level3">
<h3 class="anchored" data-anchor-id="are-any-of-these-measures-correlated">Are Any of These Measures Correlated?</h3>
<p>Spotify doesn’t just track Energy – Their team calculates various audio features for tens of millions of different songs. These audio features include a wide range of metrics that can be used to measure a song’s structure and musical content.</p>
<blockquote class="blockquote">
<p><strong>Energy</strong></p>
<p>Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.</p>
<p><strong>Danceability</strong></p>
<p>Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.</p>
<p><strong>Instrumentalness</strong></p>
<p>Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.</p>
<p><strong>Loudness</strong></p>
<p>The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.</p>
<p><strong>Speechiness</strong></p>
<p>Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g.&nbsp;talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.</p>
<p><strong>Tempo</strong></p>
<p>The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.</p>
<p><strong>Valence</strong></p>
<p>A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g.&nbsp;happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g.&nbsp;sad, depressed, angry).</p>
</blockquote>
<p>These features contain unique information about songs, but also share common characteristics in certain cases. The chart below shows how some of these measures correlate with energy for songs in the Beatles discography.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../..\posts/2024-04-08-Investigating-Beatles/images/Correlation_Chart_1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>In general, most audio features show little correlation with energy for Beatles songs. As the Spotify description notes, energetic tracks tend to be associated with things like perceived loudness or intensity, which perhaps explains why ‘loudness’ and ‘valence’ were the two metrics most correlated with energy. With valence, we observe an R-Squared of 0.25, while loudness is 0.50. That said, the observations across all measures are clearly sporadic.</p>
</section>
<section id="what-goes-into-a-rating" class="level3">
<h3 class="anchored" data-anchor-id="what-goes-into-a-rating">What Goes Into a Rating?</h3>
<p>In 2014, Spotify acquired ‘The Echo Nest’ in a $100M deal that provided it with new audio tracking capabilities. The core methodology used to calculate Spotify’s ‘Audio Features’ originated with this acquisition. Interestingly, the concepts leveraged by Echo Nest originated even earlier with a dissertation from Tristan Jehan titled, ‘Creating Music by Listening’, which was published in 2005. The entire thesis can be found <a href="https://web.media.mit.edu/~tristan/phd/dissertation/">here</a>, but a brief overview of the Abstract can be found below</p>
<blockquote class="blockquote">
<p>Machines have the power and potential to make expressive music on their own. This thesis aims to computationally model the process of creating music using experience from listening tot examples. Our unbiased signal-based solution models the life cycle of listening, composing, and performing, turning the machine into an active musician, instead of simply an instrument. We accomplish this through an analysis-synthesis technique by combined perceptual and structural modeling of the musical surface, which leads to a minimal data representation.</p>
<p>We introduce a music cognition framework that results from the interaction of psychoacoustically grounded casual listening, a time-lag embedded feature representation, and perceptual similarity clustering. Out bottom-up analysis intends to be generic and uniform by recursively revealing metrical hierarchies and structures of pitch, rhythm, and timbre. Training is suggested for top-down unbiased supervision, and is demonstrated with the prediction of downbeat. This musical intelligence enables a range of original manipulations including song alignment, music restoration, cross-synthesis or song morphing, and ultimately the synthesis of original pieces.</p>
</blockquote>
<p>While the algorithms used have become more complex over time, the core principles outlined by Jehan are still a core part of those calculations. Spotify’s documentation unfortunately does not break down how each of the current metrics are precisely calculated. Your best bet for understanding those calculations would be Jehan’s original paper. Fortunately, there are still some sporadic examples across the web that dive into these calculations a bit deeper than the primary documentation.</p>
<p>For example, we have some information about the probability distributions of these metrics. The features are not normally distributed and some like instrumentalness, acousticness, and speechiness display a significant right skew. Mark Koh, an Engineer at Spotify, outlined some of this in a presentation given at the Monthly Music Hackathon NYC in 2018. The distribution breakdowns can be found in the chart below.</p>
<p><img src="../..\posts/2024-04-08-Investigating-Beatles/images/Spotify_API_Probability_Distribution.png" class="img-fluid"></p>
<p>Mark’s presentation goes into a lot more detail on some of these metrics (which, as a non-musician, is a bit beyond my level of understanding). You can view more from that presentation <a href="https://www.youtube.com/watch?v=goUzHd7cTuA">here</a>.</p>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>This is not meant to be an objective ranking of the <em>quality</em> of the Beatles catalog. Their discography inspires all sorts of emotions in listeners, and the relative quality (or any metric for that matter) is in the eye of the beholder. Nonetheless, it’s interesting to see the ways researchers are able to quantify and measure certain attributes of songs, even from bands as iconic as the Beatles. This approach goes beyond traditional subjective interpretation and offers new insights into the construction and impact of different songs. While it doesn’t diminish the personal and emotional connections that fans have with the music, it adds a layer of appreciation for the craftsmanship and innovation that goes into creating these tracks. More than anything, I think it helps quantify many of the lessons Glenn taught me in his class years ago.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>